{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bb89f9",
   "metadata": {},
   "source": [
    "**I think we can start here with preprocessing and building up a pipeline for classifying with a traditional model from pose estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c3b21",
   "metadata": {},
   "source": [
    "# Claases\n",
    "\n",
    "- Walking\n",
    "- Standing\n",
    "- Fast walking / jogging?\n",
    "- Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7412de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\marte\\.cache\\kagglehub\\datasets\\easonlll\\hmdb51\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "\n",
    "# Download latest version\n",
    "\n",
    "path = kagglehub.dataset_download(\"easonlll/hmdb51\")\n",
    "\n",
    "\n",
    "print(\"Path to dataset files:\", path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29479e2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'version' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\__init__.py:2475\u001b[39m\n\u001b[32m   2471\u001b[39m     torch_module_name = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[34m__name__\u001b[39m, device_type])\n\u001b[32m   2472\u001b[39m     sys.modules[torch_module_name] = module\n\u001b[32m-> \u001b[39m\u001b[32m2475\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   2476\u001b[39m     export \u001b[38;5;28;01mas\u001b[39;00m export,\n\u001b[32m   2477\u001b[39m     func \u001b[38;5;28;01mas\u001b[39;00m func,\n\u001b[32m   2478\u001b[39m     library \u001b[38;5;28;01mas\u001b[39;00m library,\n\u001b[32m   2479\u001b[39m     return_types \u001b[38;5;28;01mas\u001b[39;00m return_types,\n\u001b[32m   2480\u001b[39m )\n\u001b[32m   2481\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m cond, while_loop \u001b[38;5;28;01mas\u001b[39;00m while_loop\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\export\\__init__.py:64\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrictMinMaxConstraint\n\u001b[32m     44\u001b[39m __all__ = [\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConstraint\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDim\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUnflattenedModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Constraint, Dim, dims, ShapesCollection\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexported_program\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram, ModuleCallEntry, ModuleCallSignature\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_signature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportBackwardSignature, ExportGraphSignature\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\export\\dynamic_shapes.py:23\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     _get_node_type,\n\u001b[32m     13\u001b[39m     BUILTIN_TYPES,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     tree_map_with_path,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexported_program\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Symbol\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\export\\exported_program.py:26\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     Any,\n\u001b[32m     14\u001b[39m     Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     Union,\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autograd_not_implemented\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_library\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_class_registry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeScriptObject\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m first_call_function_nn_module_stack\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcond\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     flex_attention,\n\u001b[32m      4\u001b[39m     flex_attention_backward,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhints_wrap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hints_wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DispatchKey\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, ContextManager, Dict, List, Optional, Tuple, Union\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minductor_config\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _functionalization_reapply_views_tls \u001b[38;5;28;01mas\u001b[39;00m _reapply_views\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_inductor\\config.py:44\u001b[39m\n\u001b[32m     40\u001b[39m verbose_progress = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# use fx aot graph codegen cache\u001b[39;00m\n\u001b[32m     43\u001b[39m fx_graph_cache = (\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_FX_GRAPH_CACHE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_fbcode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# use remote fx aot graph codegen cache\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# False: Disables the cache\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# True: Enables the cache\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# None: Not set -- Off for OSS, JustKnobs based for internal\u001b[39;00m\n\u001b[32m     51\u001b[39m fx_graph_remote_cache: Optional[\u001b[38;5;28mbool\u001b[39m] = fx_graph_remote_cache_default()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marte\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_inductor\\config.py:9\u001b[39m, in \u001b[36mis_fbcode\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_fbcode\u001b[39m() -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mgit_version\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch' has no attribute 'version' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm # Good practice for progress bars\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "ROOT_DIR = r\"/kaggle/input/hmdb51/HMDB51\"\n",
    "TARGET_CLASSES = [\"walk\", \"stand\", \"run\"]\n",
    "FRAMES_PER_CLIP = 8  # Common and fast choice for initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Custom HMDB51 Dataset Class ---\n",
    "class HMDB51FrameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A minimal PyTorch Dataset for HMDB51 frames, using uniform frame sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, class_list, n_frames=8, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.class_list = class_list\n",
    "        self.n_frames = n_frames\n",
    "        self.transform = transform\n",
    "        self.data_samples = []\n",
    "\n",
    "        # Map class names to integer labels (0, 1, 2, 3)\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(class_list)}\n",
    "        print(\"Mapping directory names to labels:\", self.class_to_idx)\n",
    "        \n",
    "        # --- Pre-cache all sample directories (Fast Setup) ---\n",
    "        # This is fast because it only reads the directory names once.\n",
    "        for class_name in class_list:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            # Use glob to find all sample directories (e.g., walk_001_1, walk_002_1)\n",
    "            for sample_dir in glob.glob(os.path.join(class_path, '*')):\n",
    "                if os.path.isdir(sample_dir):\n",
    "                    label = self.class_to_idx[class_name]\n",
    "                    self.data_samples.append((sample_dir, label))\n",
    "\n",
    "        print(f\"Loaded {len(self.data_samples)} total video samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_dir, label = self.data_samples[idx]\n",
    "        \n",
    "        # Find all JPG frames in the sample directory\n",
    "        frame_files = sorted(glob.glob(os.path.join(sample_dir, '*.jpg')))\n",
    "        \n",
    "        if not frame_files:\n",
    "             # Handle empty directories gracefully\n",
    "             raise RuntimeError(f\"No frames found in directory: {sample_dir}\")\n",
    "\n",
    "        # --- Minimal and Fast Frame Sampling ---\n",
    "        # Select N frames uniformly across the video sequence\n",
    "        indices = torch.linspace(0, len(frame_files) - 1, self.n_frames).long()\n",
    "        \n",
    "        frames = []\n",
    "        for i in indices:\n",
    "            frame_path = frame_files[i.item()]\n",
    "            # Open image using PIL\n",
    "            img = Image.open(frame_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            frames.append(img)\n",
    "\n",
    "        # Stack the frames: (T, C, H, W) -> (C, T, H, W) or (C, H, W) for 2D-CNN\n",
    "        # Since MobileNet is a 2D-CNN, we pass frames one by one in a batch.\n",
    "        # For this setup, we average or take a single frame per sample for simplicity.\n",
    "        # However, for a proper 3D-CNN/TSN setup, we would return the stacked clip.\n",
    "        \n",
    "        # For MobileNet, we will average the sampled frames to create one 'representative' image.\n",
    "        # This is a very common trick for 2D-CNN transfer learning on videos.\n",
    "        clip_tensor = torch.stack(frames) # (T, C, H, W)\n",
    "        final_input = torch.mean(clip_tensor, dim=0) # (C, H, W) - Mean of all frames\n",
    "        \n",
    "        return final_input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc022ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping directory names to labels: {'walk': 0, 'stand': 1, 'run': 2}\n",
      "Loaded 934 total video samples.\n",
      "\n",
      "DataLoader ready to serve batches of size 32.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Instantiate Dataset and DataLoader ---\n",
    "\n",
    "# Standard normalization for models pre-trained on ImageNet\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224), # MobileNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create the dataset instance\n",
    "hmdb_dataset = HMDB51FrameDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    class_list=TARGET_CLASSES,\n",
    "    n_frames=FRAMES_PER_CLIP,\n",
    "    transform=image_transforms\n",
    ")\n",
    "\n",
    "DATASET_SIZE = len(hmdb_dataset)\n",
    "train_size = int(0.8 * DATASET_SIZE) # 80% for training\n",
    "test_size = DATASET_SIZE - train_size # 20% for testing\n",
    "\n",
    "# Split the dataset randomly\n",
    "train_dataset, test_dataset = random_split(hmdb_dataset, [train_size, test_size])\n",
    "\n",
    "print(f\"Data Split: Training ({train_size}) / Testing ({test_size})\")\n",
    "\n",
    "\n",
    "# Create the DataLoaders for batching and shuffling\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 1. DataLoader for Training (SHUFFLED)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,           # Shuffle for training is CRITICAL\n",
    "    num_workers=4 \n",
    ")\n",
    "\n",
    "# 2. DataLoader for Testing (NOT SHUFFLED)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,          # Do not shuffle test data\n",
    "    num_workers=4 \n",
    ")\n",
    "\n",
    "print(f\"Created two DataLoaders: train_loader and test_loader.\")\n",
    "\n",
    "print(f\"\\nDataLoader ready to serve batches of size {BATCH_SIZE}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting 3 classes: ['walk', 'stand', 'run']\n",
      "\n",
      "âœ… Successfully loaded MobileNetV3-Large, pre-trained on ImageNet.\n",
      "   Modified final classification layer from 1000 outputs to 3.\n",
      "\n",
      "Model Total Parameters: 4,205,875\n",
      "Model Trainable Parameters: 1,233,923 (Only the new layer)\n",
      "Model is ready for fine-tuning on your HMDB51 data.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the target classes\n",
    "# These are the four actions you want to classify:\n",
    "TARGET_CLASSES = [\"walk\", \"stand\", \"run\"]\n",
    "NUM_CLASSES = len(TARGET_CLASSES)\n",
    "\n",
    "print(f\"Targeting {NUM_CLASSES} classes: {TARGET_CLASSES}\")\n",
    "\n",
    "# --- Load the Pre-trained Model ---\n",
    "## We use the 'large' version of MobileNetV3, which is still lightweight but powerful.\n",
    "## We specify the best available weights (IMAGENET1K_V2) which contain the transfer knowledge.\n",
    "try:\n",
    "    # 2. Load the model pre-trained on ImageNet\n",
    "    model = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V2)\n",
    "    print(\"\\n Successfully loaded MobileNetV3-Large, pre-trained on ImageNet.\")\n",
    "\n",
    "    # --- Transfer Learning Modification ---\n",
    "    # The classification head of MobileNetV3 is stored in the 'classifier' attribute.\n",
    "    # The last layer is an nn.Linear layer that outputs 1000 classes (for ImageNet).\n",
    "\n",
    "    # 3. Get the input features of the last layer\n",
    "    # We need to know how many features the MobileNet backbone outputs (usually 1280 for v3 large)\n",
    "    in_features = model.classifier[3].in_features\n",
    "\n",
    "    # 4. Replace the final layer with a new one for your 4 classes\n",
    "    # We keep the model's structure but replace the classification head.\n",
    "    model.classifier[3] = nn.Linear(in_features, NUM_CLASSES)\n",
    "    print(f\"   Modified final classification layer from 1000 outputs to {NUM_CLASSES}.\")\n",
    "\n",
    "    # --- Freeze the Backbone for Efficient Fine-Tuning ---\n",
    "    # 5. Freeze all layers except the new classification head\n",
    "    # This is a common practice in transfer learning to speed up training\n",
    "    # and prevent the pre-trained weights from being destroyed early on.\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # 6. Unfreeze the parameters of the new classification head\n",
    "    # These are the only weights that will be updated during the initial training phase.\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Check the model structure and parameter count\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"\\nModel Total Parameters: {total_params:,}\")\n",
    "    print(f\"Model Trainable Parameters: {trainable_params:,} (Only the new layer)\")\n",
    "    print(\"Model is ready for fine-tuning on the HMDB51 data.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n An error occurred during model loading: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e94dbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507bfe8",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cff08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 1. Define Training Parameters ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Loss Function: Cross-Entropy is standard for classification\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m criterion = \u001b[43mnn\u001b[49m.CrossEntropyLoss()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Optimizer: Adam is a great default choice for transfer learning\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# We only pass the trainable parameters (the final layer)\u001b[39;00m\n\u001b[32m      8\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Training Parameters ---\n",
    "\n",
    "# Loss Function: Cross-Entropy is standard for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: Adam is a great default choice for transfer learning\n",
    "# We only pass the trainable parameters (the final layer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "EPOCHS = 5 # Start with a small number of passes\n",
    "\n",
    "# --- 2. The Training Loop ---\n",
    "print(\"\\n--- Starting Training Loop ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Wrap train_loader with tqdm for a progress bar\n",
    "    # 'train_loader' is the DataLoader you successfully created in the previous step\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "        \n",
    "        # 1. Move data to the appropriate device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 2. Zero the parameter gradients (clear old gradients)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3. Forward pass (get model predictions)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 4. Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 5. Backward pass (calculate gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # 6. Optimize (update the weights of the new layer)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Calculate and print epoch statistics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} complete. Average Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"\\n Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a5ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
