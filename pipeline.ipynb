{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a5bb89f9",
      "metadata": {
        "id": "a5bb89f9"
      },
      "source": [
        "**I think we can start here with preprocessing and building up a pipeline for classifying with a traditional model from pose estimation**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "mqDn3241-eU4",
        "outputId": "f3cc4d16-54ef-40ba-8fb8-0ebcfa34aa78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mqDn3241-eU4",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f81c3b21",
      "metadata": {
        "id": "f81c3b21"
      },
      "source": [
        "# Claases\n",
        "\n",
        "- Walking\n",
        "- Standing\n",
        "- Fast walking / jogging?\n",
        "- Running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5f3cb01e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f3cb01e",
        "outputId": "f24bd882-b371-48d8-ec84-0d812fd7253c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'hmdb51' dataset.\n",
            "Path to dataset files: /kaggle/input/hmdb51\n"
          ]
        }
      ],
      "source": [
        "# import kagglehub\n",
        "# Download latest version from kagglehub:\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "\n",
        "path = kagglehub.dataset_download(\"easonlll/hmdb51\")\n",
        "\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount to my Drive\n",
        "\n",
        "I'm just doing this here because I got tired of downloading the data again and again each time the runtime environment resets."
      ],
      "metadata": {
        "id": "TcD7KwCSCEmI"
      },
      "id": "TcD7KwCSCEmI"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, models, applications\n",
        "import random\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "\n",
        "# Dataset and training configuration\n",
        "DATASET_PATH = path + \"/HMDB51\"\n",
        "TARGET_CLASSES = [\"run\", \"walk\", \"stand\"]\n",
        "FRAME_COUNT = 16\n",
        "IMAGE_SIZE = (160, 160)\n",
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "mPRI6iRP73nL"
      },
      "id": "mPRI6iRP73nL",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define generator"
      ],
      "metadata": {
        "id": "lEhMta3E9GtS"
      },
      "id": "lEhMta3E9GtS"
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoSequenceGenerator(tf.keras.utils.Sequence):\n",
        "    \"\"\"\n",
        "    Generates batches of videos for training.\n",
        "    Handles short videos via last-frame padding.\n",
        "    Returns integer labels (sparse) for sparse_categorical_crossentropy.\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_paths, labels, batch_size=8, frame_count=16, image_size=(160,160), shuffle=True):\n",
        "        self.sample_paths = sample_paths\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.frame_count = frame_count\n",
        "        self.image_size = image_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(self.sample_paths))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        # number of batches per epoch\n",
        "        return int(np.ceil(len(self.sample_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X = np.zeros((len(batch_indexes), self.frame_count, *self.image_size, 3), dtype=np.float32)\n",
        "        y = np.zeros((len(batch_indexes),), dtype=np.int32)\n",
        "\n",
        "        for i, idx in enumerate(batch_indexes):\n",
        "            video_path = self.sample_paths[idx]\n",
        "            frames = sorted([f for f in os.listdir(video_path) if f.lower().endswith(('.jpg','.png','.jpeg'))])\n",
        "\n",
        "            selected = []\n",
        "            while len(selected) < self.frame_count and len(frames) > 0:\n",
        "                selected += frames\n",
        "            selected = selected[:self.frame_count]\n",
        "\n",
        "            # load frames\n",
        "            video_frames = []\n",
        "            for fname in selected:\n",
        "                img = cv2.imread(os.path.join(video_path, fname))\n",
        "                img = cv2.resize(img, self.image_size)\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
        "                video_frames.append(img)\n",
        "\n",
        "            video_frames = np.array(video_frames, dtype=np.float32)\n",
        "\n",
        "            # pad if too short\n",
        "            if len(video_frames) < self.frame_count:\n",
        "                last_frame = video_frames[-1] if len(video_frames)>0 else np.zeros((*self.image_size,3))\n",
        "                missing = self.frame_count - len(video_frames)\n",
        "                pad = np.repeat(last_frame[np.newaxis, ...], missing, axis=0)\n",
        "                video_frames = np.concatenate([video_frames, pad], axis=0)\n",
        "\n",
        "            X[i] = video_frames\n",
        "            y[i] = self.labels[idx]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n"
      ],
      "metadata": {
        "id": "JqXN0dOI9ITo"
      },
      "id": "JqXN0dOI9ITo",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data discovery"
      ],
      "metadata": {
        "id": "KRk2SPeFRmPm"
      },
      "id": "KRk2SPeFRmPm"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP: DATASET SCANNING / GATHERING PATHS & LABELS ---\n",
        "def gather_sample_dirs(dataset_root, target_classes):\n",
        "    \"\"\"\n",
        "    Scans the dataset directory and returns lists of sample folders and their labels.\n",
        "\n",
        "    Args:\n",
        "        dataset_root (str): Root path to dataset (contains folders for each class)\n",
        "        target_classes (list of str): Classes to include, e.g., [\"run\", \"walk\", \"stand\"]\n",
        "\n",
        "    Returns:\n",
        "        sample_paths (np.array): Paths to each video folder\n",
        "        sample_labels (np.array): Integer labels corresponding to classes\n",
        "    \"\"\"\n",
        "    sample_paths = []\n",
        "    sample_labels = []\n",
        "    class_map = {cls: i for i, cls in enumerate(target_classes)}\n",
        "\n",
        "    print(f\"Scanning directory: {dataset_root}\")\n",
        "\n",
        "    for class_name in target_classes:\n",
        "        class_dir = os.path.join(dataset_root, class_name)\n",
        "\n",
        "        if not os.path.isdir(class_dir):\n",
        "            print(f\"Warning: Class folder '{class_name}' NOT found at {class_dir}\")\n",
        "            continue\n",
        "\n",
        "        label_id = class_map[class_name]\n",
        "\n",
        "        # Every video is stored as a folder of frames\n",
        "        for video_folder in os.listdir(class_dir):\n",
        "            video_path = os.path.join(class_dir, video_folder)\n",
        "            if os.path.isdir(video_path):\n",
        "                sample_paths.append(video_path)\n",
        "                sample_labels.append(label_id)\n",
        "\n",
        "    sample_paths = np.array(sample_paths)\n",
        "    sample_labels = np.array(sample_labels)\n",
        "\n",
        "    print(f\"\\n--- RESULTS ---\")\n",
        "    print(f\"Total samples found: {len(sample_paths)}\")\n",
        "    if len(sample_paths) > 0:\n",
        "        print(f\"Example path: {sample_paths[0]}\")\n",
        "        print(f\"Example label: {sample_labels[0]} ({target_classes[sample_labels[0]]})\")\n",
        "\n",
        "    return sample_paths, sample_labels\n",
        "\n",
        "\n",
        "# --- EXECUTE THE SCAN ---\n",
        "X_all, y_all = gather_sample_dirs(DATASET_PATH, TARGET_CLASSES)\n"
      ],
      "metadata": {
        "id": "bYRV-gSwRkR6",
        "outputId": "5cb67522-a123-4883-dec2-5d2d0e734414",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bYRV-gSwRkR6",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning directory: /kaggle/input/hmdb51/HMDB51\n",
            "\n",
            "--- RESULTS ---\n",
            "Total samples found: 934\n",
            "Example path: /kaggle/input/hmdb51/HMDB51/run/THE_PROTECTOR_run_f_cm_np1_le_med_42\n",
            "Example label: 0 (run)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train test split"
      ],
      "metadata": {
        "id": "GoxYTREhR9IM"
      },
      "id": "GoxYTREhR9IM"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ensure we have some data\n",
        "if len(X_all) == 0:\n",
        "    raise ValueError(\"No data found. Check your DATASET_PATH and class folders.\")\n",
        "\n",
        "# Stratified split ensures each class is proportionally represented\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_all, y_all,\n",
        "    test_size=0.2,\n",
        "    stratify=y_all,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Validation samples: {len(X_val)}\")\n"
      ],
      "metadata": {
        "id": "v_Q5uBxkR8g_",
        "outputId": "db961595-dd85-48f1-f984-7916ed9faf41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "v_Q5uBxkR8g_",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 747\n",
            "Validation samples: 187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize generator and check shape"
      ],
      "metadata": {
        "id": "XVJg2VtG9NJA"
      },
      "id": "XVJg2VtG9NJA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "train_gen = VideoSequenceGenerator(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    frame_count=FRAME_COUNT,\n",
        "    image_size=IMAGE_SIZE  # <- pass a tuple (height, width)\n",
        ")\n",
        "\n",
        "val_gen = VideoSequenceGenerator(\n",
        "    X_val,\n",
        "    y_val,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    frame_count=FRAME_COUNT,\n",
        "    image_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "# --- DEBUG CHECK ---\n",
        "# Grab the first batch to verify shapes\n",
        "try:\n",
        "    X_sample, y_sample = train_gen.__getitem__(0)\n",
        "    print(\"Generator Check Passed!\")\n",
        "    print(f\"Input Shape (Batch, Frames, H, W, Ch): {X_sample.shape}\")\n",
        "    print(f\"Labels Shape: {y_sample.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Generator Failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt7-oXgp9QIv",
        "outputId": "334fd8b0-615e-403b-a5d0-b94f85a23570"
      },
      "id": "Wt7-oXgp9QIv",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Check Passed!\n",
            "Input Shape (Batch, Frames, H, W, Ch): (8, 16, 160, 160, 3)\n",
            "Labels Shape: (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ra8fS4kGWB9_"
      },
      "id": "ra8fS4kGWB9_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap Generators as tf.data.Dataset"
      ],
      "metadata": {
        "id": "7AoOzAy-WEzW"
      },
      "id": "7AoOzAy-WEzW"
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_tf_dataset(generator):\n",
        "    \"\"\"\n",
        "    Wraps a Keras Sequence into a tf.data.Dataset that repeats indefinitely.\n",
        "    \"\"\"\n",
        "    def gen():\n",
        "        for i in range(len(generator)):\n",
        "            yield generator[i]\n",
        "\n",
        "    output_signature = (\n",
        "        tf.TensorSpec(shape=(None, FRAME_COUNT, IMAGE_SIZE[0], IMAGE_SIZE[1], 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "    )\n",
        "\n",
        "    ds = tf.data.Dataset.from_generator(gen, output_signature=output_signature)\n",
        "    ds = ds.repeat()  # repeat indefinitely for fit()\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n"
      ],
      "metadata": {
        "id": "kBYTA69dWBas"
      },
      "id": "kBYTA69dWBas",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = convert_to_tf_dataset(train_gen)\n",
        "val_ds   = convert_to_tf_dataset(val_gen)\n"
      ],
      "metadata": {
        "id": "PN91XjVxWQwc"
      },
      "id": "PN91XjVxWQwc",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model build (pre-trained efficientnet)"
      ],
      "metadata": {
        "id": "Xd7yUrLq9TPG"
      },
      "id": "Xd7yUrLq9TPG"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tunable_model():\n",
        "    video_input = layers.Input(shape=(FRAME_COUNT, *IMAGE_SIZE, 3))\n",
        "    base_cnn = applications.EfficientNetB0(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(*IMAGE_SIZE, 3)\n",
        "    )\n",
        "\n",
        "    # fine-tune top 20 layers\n",
        "    base_cnn.trainable = True\n",
        "    for layer in base_cnn.layers[:-20]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    encoded_frames = layers.TimeDistributed(base_cnn)(video_input)\n",
        "    encoded_frames = layers.TimeDistributed(layers.GlobalAveragePooling2D())(encoded_frames)\n",
        "    video_summary = layers.GlobalAveragePooling1D()(encoded_frames)\n",
        "\n",
        "    output = layers.Dense(len(TARGET_CLASSES), activation='softmax')(video_summary)\n",
        "    model = models.Model(inputs=video_input, outputs=output)\n",
        "    model.compile(\n",
        "        optimizer=Adam(1e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_tunable_model()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "69XSSHUxSgAq",
        "outputId": "95619e48-580e-4d48-a411-7c21e478dbd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "id": "69XSSHUxSgAq",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m160\u001b[0m, \u001b[38;5;34m160\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1280\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m3,843\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,053,414\u001b[0m (15.46 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,053,414</span> (15.46 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,354,803\u001b[0m (5.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,354,803</span> (5.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,698,611\u001b[0m (10.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,698,611</span> (10.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model on our classes"
      ],
      "metadata": {
        "id": "T_GLNAjJSp5n"
      },
      "id": "T_GLNAjJSp5n"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Mount Google Drive first\n",
        "\n",
        "# steps per epoch = number of batches in generator\n",
        "train_steps = len(train_gen)\n",
        "val_steps = len(val_gen)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=4,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.save(\"hmdb51_efficientnet.h5\")\n",
        "# Save the trained model to Drive\n",
        "model.save(\"/content/drive/MyDrive/hmdb51_efficientnet.h5\")"
      ],
      "metadata": {
        "id": "jmDGwR7JSr0D",
        "outputId": "dcf91d9d-5c46-4be5-d473-e75e78e7fc3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jmDGwR7JSr0D",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/15\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m805s\u001b[0m 5s/step - accuracy: 0.5275 - loss: 0.9817 - val_accuracy: 0.6043 - val_loss: 0.7801\n",
            "Epoch 2/15\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 923ms/step - accuracy: 0.7297 - loss: 0.6474 - val_accuracy: 0.6578 - val_loss: 0.7294\n",
            "Epoch 3/15\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 559ms/step - accuracy: 0.8096 - loss: 0.5100 - val_accuracy: 0.6791 - val_loss: 0.7165\n",
            "Epoch 4/15\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 425ms/step - accuracy: 0.8739 - loss: 0.3977 - val_accuracy: 0.6738 - val_loss: 0.7261\n",
            "Epoch 5/15\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 428ms/step - accuracy: 0.9246 - loss: 0.3031 - val_accuracy: 0.6631 - val_loss: 0.7498\n",
            "Epoch 6/15\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 427ms/step - accuracy: 0.9749 - loss: 0.2273 - val_accuracy: 0.6524 - val_loss: 0.7865\n",
            "Epoch 7/15\n",
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 428ms/step - accuracy: 0.9946 - loss: 0.1644 - val_accuracy: 0.6417 - val_loss: 0.8369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(\"hmdb51_efficientnet.h5\")"
      ],
      "metadata": {
        "id": "V8CzQikRG08n"
      },
      "id": "V8CzQikRG08n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# --- 1. Preload all validation videos into memory ---\n",
        "X_val_all = []\n",
        "y_val_all = []\n",
        "\n",
        "for i in range(len(val_gen)):\n",
        "    X_batch, y_batch = val_gen[i]\n",
        "    X_val_all.append(X_batch)\n",
        "    y_val_all.append(y_batch)\n",
        "\n",
        "X_val_all = np.concatenate(X_val_all, axis=0)\n",
        "y_val_all = np.concatenate(y_val_all, axis=0)\n",
        "\n",
        "# --- 2. Predict all at once ---\n",
        "preds = model.predict(X_val_all, verbose=1)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "y_true = y_val_all\n",
        "\n",
        "# --- 3. Confusion matrix ---\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Optional nicer display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=TARGET_CLASSES, yticklabels=TARGET_CLASSES)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Validation Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_true, y_pred, target_names=TARGET_CLASSES))\n"
      ],
      "metadata": {
        "id": "G6iZtnb5uw7x",
        "outputId": "708c7ecd-98ff-457d-d1f5-90d17674ff7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "id": "G6iZtnb5uw7x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'val_gen' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1203036207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_val_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mX_val_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_gen' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}